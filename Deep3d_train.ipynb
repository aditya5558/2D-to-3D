{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "691749ec180d4296af0e98708bedc5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_95c4a23c50414955b9272209d22fcec0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_08417717cd2847538ed6c65f761c253a",
              "IPY_MODEL_aefba181d0884a79975471c06b5a2701"
            ]
          }
        },
        "95c4a23c50414955b9272209d22fcec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08417717cd2847538ed6c65f761c253a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6419fdea4219406cac588d517a7bfd58",
            "_dom_classes": [],
            "description": "training:   1%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 101,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ced223847abd4218908e1a4f49242734"
          }
        },
        "aefba181d0884a79975471c06b5a2701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0c6c5192f2d4847ae9e301a67103ac5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/101 [00:03&lt;05:31,  3.31s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ef3cafdc11a49e2b50c70e7f35b3529"
          }
        },
        "6419fdea4219406cac588d517a7bfd58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ced223847abd4218908e1a4f49242734": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0c6c5192f2d4847ae9e301a67103ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ef3cafdc11a49e2b50c70e7f35b3529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVSEs1DGXLyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "39e19ffa-6794-4538-d7c5-7a9c16b394f3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr  7 01:59:07 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ5s0UgOr6QU",
        "colab_type": "code",
        "outputId": "1106bc12-07cf-44c5-cf76-76acd79e6564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EReeYWPSy_fx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df8b3722-5612-41a1-ce15-63c9bf2e37e3"
      },
      "source": [
        "# !mkdir data\n",
        "# %cd data\n",
        "# !mkdir left right\n",
        "# %cd .."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hignX1yzut0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Pose/left/. ./data/left/\n",
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Person/left/. ./data/left/\n",
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Video/left/. ./data/left/\n",
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Segmentation/left/. ./data/left/\n",
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Negative/left/. ./data/left/\n",
        "\n",
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Pose/right/. ./data/right/\n",
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Person/right/. ./data/right/\n",
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Video/right/. ./data/right/\n",
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Segmentation/right/. ./data/right/\n",
        "# !cp -a /content/drive/My\\ Drive/Inria_data/Negative/right/. ./data/right/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8YGCR4_QVj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/My\\ Drive/data.zip ./\n",
        "!unzip -q data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgtLpdxry3B0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision   \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.utils.data as data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rTEeALe8gOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=(160, 384), scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.3651, 0.2656, 0.2544], [0.1847, 0.1425, 0.1418])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=(160, 384)),\n",
        "        # transforms.Resize(256),\n",
        "        # transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.3651, 0.2656, 0.2544], [0.1847, 0.1425, 0.1418])\n",
        "    ]),\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDKiqHKxu405",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Inria(data.Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        super(Inria, self).__init__()\n",
        "        self.left_files = glob.glob(os.path.join(folder_path,'left','*.jpg'))\n",
        "        self.right_files = glob.glob(os.path.join(folder_path,'left','*.jpg'))\n",
        "\n",
        "        self.transform = transform\n",
        "        # self.mask_files = []\n",
        "        # for img_path in img_files:\n",
        "        #      self.mask_files.append(os.path.join(folder_path,'mask',os.path.basename(img_path)) \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "            left_img_path = self.left_files[index]\n",
        "            right_img_path = self.right_files[index]\n",
        "\n",
        "            left = Image.open(left_img_path)\n",
        "            right = Image.open(right_img_path)\n",
        "\n",
        "            if self.transform:\n",
        "              left = self.transform(left)\n",
        "              right = self.transform(right)\n",
        "\n",
        "            # left = torchvision.transforms.ToTensor()(Image.open(left_img_path))\n",
        "            # right = torchvision.transforms.ToTensor()(Image.open(right_img_path))\n",
        "\n",
        "            # img = torchvision.transforms.ToTensor()(img)Œ\n",
        "\n",
        "            return left, right\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.left_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v4QiqJrw_e2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_path = \"./data/\"\n",
        "train_dataset = Inria(folder_path, data_transforms['train'])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5kjaRFBHf9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# img = np.transpose(train_dataset[35][0].numpy(), (1, 2, 0)) \n",
        "# plt.imshow(img)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5VaWu497bbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Deep3D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Deep3D, self).__init__()\n",
        "        \n",
        "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.relu1_1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.relu2_1 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.relu3_1 = nn.ReLU()\n",
        "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.relu3_2 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.relu4_1 = nn.ReLU()\n",
        "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.relu4_2 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.relu5_1 = nn.ReLU()\n",
        "        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.relu5_2 = nn.ReLU()\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc6 = nn.Linear(30720, 512)  \n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.drop6 = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.fc7 = nn.Linear(512, 512)  \n",
        "        self.relu7 = nn.ReLU()\n",
        "        self.drop7 = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.fc8 = nn.Linear(512, 33*12*5)\n",
        "        # rehape in forward to get pred5\n",
        "\n",
        "        self.bn_pool4 = nn.BatchNorm2d(512)\n",
        "        self.pred4 = nn.Conv2d(in_channels=512 ,out_channels=33, kernel_size=3 ,padding=1)\n",
        "        self.bn_pool3 = nn.BatchNorm2d(256)\n",
        "        self.pred3 = nn.Conv2d(in_channels=256 ,out_channels=33, kernel_size=3 ,padding=1)\n",
        "        self.bn_pool2 = nn.BatchNorm2d(128)\n",
        "        self.pred2 = nn.Conv2d(in_channels=128 ,out_channels=33, kernel_size=3 ,padding=1)\n",
        "        self.bn_pool1 = nn.BatchNorm2d(64)\n",
        "        self.pred1 = nn.Conv2d(in_channels=64 ,out_channels=33, kernel_size=3 ,padding=1)\n",
        "        \n",
        "        workspace = 0\n",
        "        scale = 1\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.deconv_pred1 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=1, padding=0, stride=1)\n",
        "        scale *= 2\n",
        "\n",
        "        self.deconv_pred2 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)\n",
        "        scale *= 2\n",
        "\n",
        "        self.deconv_pred3 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)\n",
        "        scale *= 2\n",
        "\n",
        "        self.deconv_pred4 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)\n",
        "        scale *= 2\n",
        "\n",
        "        self.deconv_pred5 = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)\n",
        "        self.relu1 =  nn.ReLU()\n",
        "        scale = 2\n",
        "        self.deconv_predup = nn.ConvTranspose2d(in_channels=33, out_channels=33, kernel_size=2*scale, padding=scale//2, stride=scale)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.convolution0 = nn.Conv2d(in_channels=33 ,out_channels=33, kernel_size=3 ,padding=1)\n",
        "\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "        # self.linear_closs = nn.Linear(hidden_sizes[5], feat_dim, bias=False)\n",
        "\n",
        "    def forward(self, x, evalMode=False):\n",
        "        out = x\n",
        "        \n",
        "        out = self.relu1_1(self.conv1_1(out))\n",
        "        pool1 = self.pool1(out)\n",
        "\n",
        "        out = self.relu2_1(self.conv2_1(pool1))\n",
        "        pool2 = self.pool2(out)\n",
        "\n",
        "        out = self.relu3_1(self.conv3_1(pool2))\n",
        "        out = self.relu3_2(self.conv3_2(out))\n",
        "        pool3 = self.pool3(out)\n",
        "\n",
        "        out = self.relu4_1(self.conv4_1(pool3))\n",
        "        out = self.relu4_2(self.conv4_2(out))\n",
        "        pool4 = self.pool4(out)\n",
        "\n",
        "        out = self.relu5_1(self.conv5_1(pool4))\n",
        "        out = self.relu5_2(self.conv5_2(out))\n",
        "        pool5 = self.pool5(out)\n",
        "\n",
        "        out = self.flatten(pool5)\n",
        "\n",
        "        out = self.drop6(self.relu6(self.fc6(out)))\n",
        "        out = self.drop7(self.relu7(self.fc7(out)))\n",
        "\n",
        "        out = self.fc8(out)\n",
        "\n",
        "        pred5 = torch.reshape(out, (out.shape[0], 33, 5, 12))\n",
        "\n",
        "        pred4 = self.bn_pool4(pool4)\n",
        "        pred4 = self.pred4(pred4)\n",
        "        pred3 = self.bn_pool3(pool3)\n",
        "        pred3 = self.pred3(pred3)\n",
        "        pred2 = self.bn_pool2(pool2)\n",
        "        pred2 = self.pred2(pred2)\n",
        "        pred1 = self.bn_pool1(pool1)\n",
        "        pred1 = self.pred1(pred1)\n",
        "\n",
        "        pred1 = self.deconv_pred1(self.relu(pred1))\n",
        "        pred2 = self.deconv_pred2(self.relu(pred2))\n",
        "        pred3 = self.deconv_pred3(self.relu(pred3))\n",
        "        pred4 = self.deconv_pred4(self.relu(pred4))\n",
        "        pred5 = self.deconv_pred5(self.relu(pred5))\n",
        "\n",
        "        pred = pred1 + pred2 + pred3 + pred4 + pred5\n",
        "        pred = self.relu(pred)\n",
        "\n",
        "        pred = self.convolution0(self.relu(self.deconv_predup(pred)))\n",
        "\n",
        "        mask = self.softmax(pred)\n",
        "\n",
        "        return mask\n",
        "\n",
        "def selection_layer(masks, left_image, left_shift=16):\n",
        "\n",
        "    p2d = (left_shift-1, left_shift, 0, 0)\n",
        "    padded_img = F.pad(left_image, p2d, 'constant')\n",
        "\n",
        "    depth = masks.shape[1]\n",
        "    width = left_image.shape[3]\n",
        "    layers = []\n",
        "    layers.append(torch.zeros(padded_img[:,:,:,:width].shape))\n",
        "    for d in range(depth-2,-1,-1):\n",
        "        layers.append(padded_img[:,:,:,d:d+width])\n",
        "    layers = torch.stack(layers, axis=1)\n",
        "    disparity_image =  layers * masks.unsqueeze(2)\n",
        "\n",
        "    return torch.sum(disparity_image, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rweG_DBj7suc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6ec42ac-5b2f-4913-846b-d91b2e80e1e1"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device='cpu'\n",
        "print(device)\n",
        "\n",
        "net = Deep3D()\n",
        "net = net.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWXFKEz475XD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26ceb382-6c85-4fc2-8c02-57a6ebc9b74b"
      },
      "source": [
        "checkpoint = torch.load(\"/content/drive/My Drive/pre_trained_weights.pt\")\n",
        "net.load_state_dict(checkpoint[\"model_state_dict\"])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAzP_ZaYGAJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data_loader, n_epochs, criterion):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        avg_loss = 0.0\n",
        "        num_correct = 0.0\n",
        "        total = 0.0\n",
        "\n",
        "        outer = tqdm.notebook.tqdm(total=len(data_loader), desc='training')\n",
        "\n",
        "        for batch_num, (left_img, right_img) in enumerate(data_loader):\n",
        "            outer.update(1)\n",
        "            left_img, right_img = left_img.to(device), right_img.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(left_img)\n",
        "            outputs = selection_layer(outputs, left_img)\n",
        "\n",
        "            loss = criterion(outputs, right_img)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            if batch_num % 100 == 99:\n",
        "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}\\tAcc: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/total, num_correct/total))\n",
        "                # avg_loss = 0.0    \n",
        "            \n",
        "            torch.cuda.empty_cache()\n",
        "            del left_img\n",
        "            del right_img\n",
        "            del loss\n",
        "\n",
        "        torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_label_state_dict': optimizer.state_dict(),\n",
        "        }, '/content/drive/My Drive/11785/' + 'weight' + '_' + str(epoch) + '.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icsSYAlNU-v3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "691749ec180d4296af0e98708bedc5c4",
            "95c4a23c50414955b9272209d22fcec0",
            "08417717cd2847538ed6c65f761c253a",
            "aefba181d0884a79975471c06b5a2701",
            "6419fdea4219406cac588d517a7bfd58",
            "ced223847abd4218908e1a4f49242734",
            "e0c6c5192f2d4847ae9e301a67103ac5",
            "7ef3cafdc11a49e2b50c70e7f35b3529"
          ]
        },
        "outputId": "3b0b2395-7fa5-43e9-e84a-fa89d8b1849f"
      },
      "source": [
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
        "\n",
        "train(net, train_dataloader, 1, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "691749ec180d4296af0e98708bedc5c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='training', max=101, style=ProgressStyle(description_width='in…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8jmOmffF9rT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, img_path):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      shape = (384, 160)\n",
        "      input_img = cv2.imread(img_path)\n",
        "      input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
        "      input_img = cv2.resize(input_img, (384, 160))\n",
        "      left = input_img\n",
        "      plt.imshow(input_img)\n",
        "      plt.show()\n",
        "\n",
        "      input_img = input_img.astype(np.float32)\n",
        "      input_img = np.rollaxis(input_img, 2, 0)\n",
        "      input_img = np.reshape(input_img, (1, 3, 160, 384))  \n",
        "      # input_img = input_img.transpose((1, 3, 160, 384))\n",
        "      input_img = torch.FloatTensor(input_img)\n",
        "      input_img = input_img.to(device)\n",
        "\n",
        "      mask = net(input_img)\n",
        "\n",
        "      right_image = selection_layer(mask, input_img).detach().cpu().numpy()\n",
        "      print(right_image.shape)\n",
        "      right_image = np.clip(right_image.squeeze().transpose((1,2,0)), 0, 255).astype(np.uint8)\n",
        "      right = right_image\n",
        "    #   print(right_image.shape)\n",
        "      plt.imshow(right_image)\n",
        "      plt.show()\n",
        "      right_image = Image.fromarray(cv2.cvtColor(right_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      print(input_img.shape)\n",
        "      left_image = np.clip(input_img.numpy().squeeze().transpose((1,2,0)), 0, 255).astype(np.uint8)\n",
        "      left_image = Image.fromarray(cv2.cvtColor(left_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      right_image.save('output.jpg')\n",
        "      return left_image, right_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIHoL8BSWfSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}